# -*- coding: utf-8 -*-
"""qlalg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U2HuYXDQgdxvhBRXfzeI8bBTuj_4P6Ww
"""

import numpy as np
import random
# GridWorld setup
rows, cols = 5, 5
start = (0, 0)
goal = (4, 4)
obstacles = [(1, 1), (2, 3), (3, 1)]
# Actions and movement mappings
actions = ['U', 'D', 'L', 'R']
action_map = {'U': (-1, 0), 'D': (1, 0), 'L': (0, -1), 'R': (0, 1)}
# Initialize Q-table: [rows][cols][actions]
q_table = np.zeros((rows, cols, len(actions)))
# Hyperparameters
alpha = 0.1 # learning rate
gamma = 0.9 # discount factor
epsilon = 0.2 # exploration rate
episodes = 500 # number of training episodes
# Get reward for a given state
def get_reward(state):
    if state == goal:
        return 100
    elif state in obstacles:
        return -100
    else:
        return -1
# Check if a position is valid (within bounds and not an obstacle)
def is_valid(pos):
    r, c = pos
    return 0 <= r < rows and 0 <= c < cols and pos not in obstacles
# Q-learning Training loop
for ep in range(episodes):
    state = start
    while state != goal:
        r, c = state
        # Choose action using epsilon-greedy strategy
        if random.random() < epsilon:
            a = random.randint(0, 3) # Explore
        else:
            a = np.argmax(q_table[r, c]) # Exploit
        action = actions[a]
        dr, dc = action_map[action]
        new_r, new_c = r + dr, c + dc
        new_state = (new_r, new_c)
        if not is_valid(new_state):
            new_state = state # Stay in place if move is invalid
        # Q-value update
        reward = get_reward(new_state)
        max_q = np.max(q_table[new_state[0], new_state[1]])
        q_table[r, c, a] += alpha * (reward + gamma * max_q - q_table[r, c, a])
        state = new_state
# Build policy grid for display
policy_grid = np.full((rows, cols), ' ')
for r in range(rows):
    for c in range(cols):
        if (r, c) == goal:
            policy_grid[r, c] = 'G'
        elif (r, c) == start:
            policy_grid[r, c] = 'S'
        elif (r, c) in obstacles:
            policy_grid[r, c] = 'X'
        else:
            best_a = np.argmax(q_table[r, c])
            policy_grid[r, c] = actions[best_a]
# Display optimal policy
print("\n Learned Optimal Policy:")
for row in policy_grid:
    print(' '.join(row))
# Show the final path from start to goal
print("\n Agent path from Start to Goal:")
state = start
path = [state]
while state != goal:
    r, c = state
    best_a = np.argmax(q_table[r, c])
    dr, dc = action_map[actions[best_a]]
    new_state = (r + dr, c + dc)
    if not is_valid(new_state) or new_state in path:
        break # Stop if stuck or looping
    path.append(new_state)
    state = new_state
print(" â†’ ".join(str(p) for p in path))

